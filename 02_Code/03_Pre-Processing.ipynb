{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Section objectives\n",
    "I this notebook, I document the pre-processing of the data before going on to modelling. To do so, I first separate the whole dataset into train (80%) and test (20%) data subsets. Thereafter, I perform a number of methods on the training dataset, which I then use to pre-process the test dataset with the parameters of the train data. The last two steps are concerned with defining the validation methods for recursive feature selection and k-fold cross-validation.\n",
    "\n",
    "Best practice pre-processing is **typically done within the modelling to avoid overfitting the models during cross-validation**, but I decided to make it a separate step for two reasons: 1) I wanted to review the data after each step and add more (and more robust) methods to the pre-processing, which is very cumbersome to do within the modelling step. 2) The critical performance metrics will later be obtained from the test dataset which in fact will never have seen any data from the train dataset before trying to predict performance outcomes and generating confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the work environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Set general options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed\n",
    "set.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General options\n",
    "options(scipen = 999,\n",
    "        readr.num_columns = 0,\n",
    "        warn=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set wd\n",
    "setwd(\"C:/Users/veren/github/ML_Project_Predict_Employee_Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "-- Attaching packages --------------------------------------- tidyverse 1.3.0 --\n",
      "v tibble  3.0.1     v dplyr   0.8.5\n",
      "v tidyr   1.0.3     v stringr 1.4.0\n",
      "v readr   1.3.1     v forcats 0.5.0\n",
      "v purrr   0.3.4     \n",
      "-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x dplyr::filter() masks stats::filter()\n",
      "x dplyr::lag()    masks stats::lag()\n",
      "x purrr::lift()   masks caret::lift()\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Import user-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load functions\n",
    "load(\"03_Objects/ud_functions.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Import workspace from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load functions\n",
    "load(\"03_Objects/eda.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Separate into train and test\n",
    "In this step, the dataset is separated into two subsets, train and test. On the train dataset, I will perform all modelling and cross-validation. The test dataset will serve to evaluate the models by predicting values for a dataset the model has never seen before, and perform Out-Of-Sample validation on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate data into train (80%; to have sufficient data for modelling) and test (20%)\n",
    "split <- caret::createDataPartition(perf$performance, p = .8, list = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train_raw\n",
    "train_raw <- perf[split, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test_raw\n",
    "test_raw <- perf[-split, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save test raw\n",
    "write.csv(test_raw, \"01_Data/test_raw.csv\")   #this dataset is saved to make possible standalone predictions later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-processing train data\n",
    "\n",
    "Data pre-processing is handled for train and test datasets separately. To avoid any training effects on the test dataset, we will save the parameters found from the train dataset and then use them to perform the same pre-processing steps on the test dataset later on.\n",
    "\n",
    "Steps for train:\n",
    "1. Separate data into target and predictors (this makes it easier to perform One-Hot-Encoding which is actually not of use here, but I wanted to keep the code anyway in case someone wants to try it with categorical predictors)\n",
    "2. One-Hot-Encoding for categorical predictors (not necessary here)\n",
    "3. Robust winsorizing of predictors (with robust z-values + MAD) to adjust extreme univariate values (threshold alpha = .01) and avoid issues during later centering and scaling\n",
    "4. Re-integrate pre-processed predictors with target variable\n",
    "5. Use Mahalanobis Distance to inspect (and if necessary, delete) extreme multivariate values, at alpha = .001\n",
    "6. Use preProcess = c(\"zv\", \"nzv\", \"corr\", \"center\", \"scale\", \"knnImpute\"). Those methods will delete predictors with zero or near-zero variance, remove highly correlated predictors, center and scale all predictors and impute missing values using KNN algorithm).\n",
    "\n",
    "Steps for test (see in notebook 05_Model Evaluation):\n",
    "1. Separate data into target and predictors\n",
    "2. One-Hot-Encoding for categorical predictors (not necessary here)\n",
    "3. Robust winsorizing of predictors (with robust z-values + MAD parameters from train) to adjust extreme univariate values (threshold alpha = .01)\n",
    "4. Re-integrate pre-processed predictors with target variable\n",
    "5. Use preProcess (parameters of train)\n",
    "\n",
    "Note that Mahalanobis' distance is not performed on the test dataset. This is because I use MD on the train dataset to find and delete multivariate outliers, thus avoiding that extreme cases will lead to overfitting the models. In the test dataset, entire observations can never be deleted, which is why I skip this step during pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save target separately\n",
    "target_train <- train_raw$performance\n",
    "train <- train_raw[, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    quality      service_orientation   innovation     organisation \n",
       " Min.   :2.000   Min.   :2.000       Min.   :1.000   Min.   :2.00  \n",
       " 1st Qu.:3.000   1st Qu.:3.000       1st Qu.:3.000   1st Qu.:3.00  \n",
       " Median :3.000   Median :4.000       Median :3.000   Median :3.00  \n",
       " Mean   :3.489   Mean   :3.709       Mean   :3.231   Mean   :3.21  \n",
       " 3rd Qu.:4.000   3rd Qu.:4.000       3rd Qu.:4.000   3rd Qu.:4.00  \n",
       " Max.   :5.000   Max.   :5.000       Max.   :5.000   Max.   :5.00  \n",
       "                                                     NA's   :6     \n",
       " problem_solving   curiosity      determination       analysis     \n",
       " Min.   :2.000   Min.   :0.4600   Min.   :0.5400   Min.   :0.2100  \n",
       " 1st Qu.:3.000   1st Qu.:0.7500   1st Qu.:0.8225   1st Qu.:0.6025  \n",
       " Median :4.000   Median :0.8700   Median :0.9400   Median :0.7700  \n",
       " Mean   :3.529   Mean   :0.8435   Mean   :0.8973   Mean   :0.7508  \n",
       " 3rd Qu.:4.000   3rd Qu.:0.9700   3rd Qu.:1.0000   3rd Qu.:0.9200  \n",
       " Max.   :5.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n",
       " NA's   :10                                                        \n",
       "  empowerment    \n",
       " Min.   :0.6000  \n",
       " 1st Qu.:0.8525  \n",
       " Median :0.9500  \n",
       " Mean   :0.9155  \n",
       " 3rd Qu.:1.0000  \n",
       " Max.   :1.0000  \n",
       "                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Process one-hot-encoding for categorical variables\n",
    "dmy <- dummyVars(~ ., data = train, levelsOnly = T, fullRank = T)\n",
    "train <- as.data.frame(predict(dmy, train))\n",
    "\n",
    "\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save parameters of train\n",
    "centers_train <- apply(train, 2, function(x) median(x, na.rm = T))\n",
    "scales_train <- apply(train, 2, mad_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    quality      service_orientation   innovation     organisation \n",
       " Min.   :2.000   Min.   :3.743       Min.   :2.743   Min.   :2.00  \n",
       " 1st Qu.:3.000   1st Qu.:3.743       1st Qu.:3.000   1st Qu.:3.00  \n",
       " Median :3.000   Median :4.000       Median :3.000   Median :3.00  \n",
       " Mean   :3.489   Mean   :3.936       Mean   :3.058   Mean   :3.21  \n",
       " 3rd Qu.:4.000   3rd Qu.:4.000       3rd Qu.:3.257   3rd Qu.:4.00  \n",
       " Max.   :5.000   Max.   :4.257       Max.   :3.257   Max.   :5.00  \n",
       "                                                     NA's   :6     \n",
       " problem_solving   curiosity      determination       analysis     \n",
       " Min.   :2.000   Min.   :0.4600   Min.   :0.7114   Min.   :0.2100  \n",
       " 1st Qu.:3.000   1st Qu.:0.7500   1st Qu.:0.8225   1st Qu.:0.6025  \n",
       " Median :4.000   Median :0.8700   Median :0.9400   Median :0.7700  \n",
       " Mean   :3.529   Mean   :0.8435   Mean   :0.9011   Mean   :0.7508  \n",
       " 3rd Qu.:4.000   3rd Qu.:0.9700   3rd Qu.:1.0000   3rd Qu.:0.9200  \n",
       " Max.   :5.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n",
       " NA's   :10                                                        \n",
       "  empowerment    \n",
       " Min.   :0.7595  \n",
       " 1st Qu.:0.8525  \n",
       " Median :0.9500  \n",
       " Mean   :0.9222  \n",
       " 3rd Qu.:1.0000  \n",
       " Max.   :1.0000  \n",
       "                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Winsorizing (robust method with median and mad)\n",
    "train <- wins_rob(train, centers = centers_train, scales = scales_train)\n",
    "\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-integrate data with target\n",
    "train <- data.frame(performance = target_train, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 0 × 10</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>performance</th><th scope=col>quality</th><th scope=col>service_orientation</th><th scope=col>innovation</th><th scope=col>organisation</th><th scope=col>problem_solving</th><th scope=col>curiosity</th><th scope=col>determination</th><th scope=col>analysis</th><th scope=col>empowerment</th></tr>\n",
       "\t<tr><th scope=col>&lt;ord&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 0 × 10\n",
       "\\begin{tabular}{llllllllll}\n",
       " performance & quality & service\\_orientation & innovation & organisation & problem\\_solving & curiosity & determination & analysis & empowerment\\\\\n",
       " <ord> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 0 × 10\n",
       "\n",
       "| performance &lt;ord&gt; | quality &lt;dbl&gt; | service_orientation &lt;dbl&gt; | innovation &lt;dbl&gt; | organisation &lt;dbl&gt; | problem_solving &lt;dbl&gt; | curiosity &lt;dbl&gt; | determination &lt;dbl&gt; | analysis &lt;dbl&gt; | empowerment &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "\n"
      ],
      "text/plain": [
       "     performance quality service_orientation innovation organisation\n",
       "     problem_solving curiosity determination analysis empowerment"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Mahalanobis Distance (delete multivariate extreme values)\n",
    "md(train, showout = T)  #just show multivariate outliers; there are none\n",
    "\n",
    "train <- md(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pre-processing\n",
    "prep <- caret::preProcess(train, method = c(\"zv\", \"nzv\", \"corr\",\n",
    "                                            \"center\", \"scale\",\n",
    "                                            \"knnImpute\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " performance    quality        service_orientation   innovation     \n",
       " C:63        Min.   :-2.1178   Min.   :-1.1626     Min.   :-1.8693  \n",
       " B:95        1st Qu.:-0.6955   1st Qu.:-1.1626     1st Qu.:-0.3437  \n",
       " A:24        Median :-0.6955   Median : 0.3819     Median :-0.3437  \n",
       "             Mean   : 0.0000   Mean   : 0.0000     Mean   : 0.0000  \n",
       "             3rd Qu.: 0.7268   3rd Qu.: 0.3819     3rd Qu.: 1.1819  \n",
       "             Max.   : 2.1490   Max.   : 1.9264     Max.   : 1.1819  \n",
       "  organisation      problem_solving       curiosity       determination    \n",
       " Min.   :-1.65816   Min.   :-2.251706   Min.   :-2.7788   Min.   :-1.8548  \n",
       " 1st Qu.:-0.28804   1st Qu.:-0.779107   1st Qu.:-0.6776   1st Qu.:-0.7684  \n",
       " Median :-0.28804   Median : 0.546231   Median : 0.1919   Median : 0.3804  \n",
       " Mean   : 0.01459   Mean   :-0.002352   Mean   : 0.0000   Mean   : 0.0000  \n",
       " 3rd Qu.: 1.08209   3rd Qu.: 0.693491   3rd Qu.: 0.9164   3rd Qu.: 0.9670  \n",
       " Max.   : 2.45222   Max.   : 2.166090   Max.   : 1.1338   Max.   : 0.9670  \n",
       "    analysis         empowerment     \n",
       " Min.   :-2.74010   Min.   :-1.8741  \n",
       " 1st Qu.:-0.75129   1st Qu.:-0.8028  \n",
       " Median : 0.09744   Median : 0.3202  \n",
       " Mean   : 0.00000   Mean   : 0.0000  \n",
       " 3rd Qu.: 0.85750   3rd Qu.: 0.8961  \n",
       " Max.   : 1.26286   Max.   : 0.8961  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "FALSE"
      ],
      "text/latex": [
       "FALSE"
      ],
      "text/markdown": [
       "FALSE"
      ],
      "text/plain": [
       "[1] FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pre-process train\n",
    "train <- as.data.frame(predict(prep, train))\n",
    "\n",
    "\n",
    "summary(train)\n",
    "anyNA(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define cross-validation methods in caret framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Control methods for Recursive Feature Elimination\n",
    "Usually, for a dataset with so few predictors, it is not necessary to perform RFE as the primary goal is to find the handful of strongest predictors among a large predictor pool.\n",
    "\n",
    "However, as the original project contained up to 30 predictors and the challenge was to find the best predictors according to a principle of parsimony - i.e. it was deemed fundamental to achieve acceptably high predictive validity with the lowest number of predictors possible - I will show how RFE can be used to streamline the selection process of the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "rfe_nb <- rfeControl(functions = nbFuncs,\n",
    "                     method = \"repeatedcv\",\n",
    "                     repeats = 10,\n",
    "                     verbose = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Discriminant Analysis\n",
    "rfe_lda <- rfeControl(functions = ldaFuncs,\n",
    "                      method = \"repeatedcv\",\n",
    "                      repeats = 10,\n",
    "                      verbose = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "rfe_rf <- rfeControl(functions = rfFuncs,\n",
    "                     method = \"repeatedcv\",\n",
    "                     repeats = 10,\n",
    "                     verbose = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Others\n",
    "rfe_gen <- rfeControl(functions = caretFuncs,\n",
    "                      method = \"repeatedcv\",\n",
    "                      repeats = 5,\n",
    "                      verbose = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Control method for k-fold cross-validation\n",
    "\n",
    "k-fold cross-validation is a good way to estimate out-of-sample validity of the held-out sets if there are few data points to work with. In this case, I decided to do 10 times repeated 10-fold cross-validation and use SMOTE, a statistical procedure of over- and undersampling typically used for small and/or imbalanced datasets that contain few examples of the class to be predicted (which, in that case, would be our A performers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of CV method\n",
    "ctrl <- caret::trainControl(method = \"repeatedcv\", repeats = 10,\n",
    "                            selectionFunction = \"best\",\n",
    "                            summaryFunction = multiClassSummary,\n",
    "                            savePredictions = \"final\", classProbs = T,\n",
    "                            allowParallel = T,\n",
    "                            sampling = \"smote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save workspace\n",
    "To use the workspace in the next notebook, I will save it as an R object to be able to import it into other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save.image(file = \"03_Objects/pre_process.RData\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
